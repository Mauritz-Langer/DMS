{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# S&P 500 Volatilit√§tsanalyse mit GARCH-Modellen\n",
    "## Umfassende Analyse von Index- und Einzelaktiendaten\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Projekt√ºbersicht\n",
    "\n",
    "Dieses Notebook f√ºhrt eine detaillierte Volatilit√§tsanalyse des S&P 500 durch und beantwortet die zentrale **Forschungsfrage**:\n",
    "\n",
    "> *\"Inwieweit k√∂nnen GARCH-Modelle unterschiedlicher Komplexit√§t (GARCH(1,1), EGARCH, GJR-GARCH) die Volatilit√§tsdynamiken des S&P 500 Index vorhersagen, und welches Modell zeigt die beste Out-of-Sample-Prognoseperformance?\"*\n",
    "\n",
    "### üìã Notebook-Struktur\n",
    "\n",
    "- **Teil 1:** Deskriptive Analyse der S&P 500 Einzelaktiendaten\n",
    "- **Teil 2:** Index-Daten laden und Renditen berechnen\n",
    "- **Teil 3:** Stationarit√§tstest (ADF-Test)\n",
    "- **Teil 4:** GARCH-Modellierung (GARCH, EGARCH, GJR-GARCH)\n",
    "- **Teil 5:** Out-of-Sample Prognose mit Rolling Forecast\n",
    "- **Teil 6:** Modellevaluation und Vergleich\n",
    "- **Teil 7:** Visualisierung des Leverage-Effekts\n",
    "- **Teil 8:** Konfidenzintervalle f√ºr Volatilit√§tsprognosen\n",
    "- **Teil 9:** Zusammenfassung und Fazit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Installation und Abh√§ngigkeiten\n",
    "\n",
    "**Was passiert hier?**  \n",
    "Wir installieren alle ben√∂tigten Python-Bibliotheken f√ºr die Analyse:\n",
    "- `pandas` & `numpy`: Datenverarbeitung und numerische Berechnungen\n",
    "- `matplotlib` & `seaborn`: Visualisierung\n",
    "- `arch`: Spezialbibliothek f√ºr GARCH-Modelle\n",
    "- `statsmodels`: Statistische Tests (z.B. Stationarit√§t)\n",
    "- `scikit-learn`: Evaluationsmetriken (MSE, MAE)\n",
    "- `scipy`: Wissenschaftliche Berechnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pakete installieren (auskommentiert - nur bei Bedarf ausf√ºhren)\n",
    "# !pip install pandas numpy matplotlib seaborn arch statsmodels scikit-learn scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Teil 1: Deskriptive Analyse der S&P 500 Einzelaktiendaten\n",
    "\n",
    "**Ziel:** Verstehen der Datenstruktur und Qualit√§t der Einzelaktiendaten\n",
    "\n",
    "**Was macht dieser Abschnitt?**\n",
    "1. L√§dt die Datei `sp500_stocks.csv` mit historischen Aktiendaten\n",
    "2. Analysiert Datenqualit√§t (fehlende Werte, Zeitr√§ume)\n",
    "3. Berechnet deskriptive Statistiken (Quartile, Durchschnitte)\n",
    "4. Gibt einen √úberblick √ºber die Datenverteilung\n",
    "\n",
    "**Wichtige Kennzahlen:**\n",
    "- **Open/Close**: Er√∂ffnungs- und Schlusskurse der Aktien\n",
    "- **Volume**: Handelsvolumen (wie viele Aktien wurden gehandelt)\n",
    "- **Intraday Volatilit√§t**: Differenz zwischen H√∂chst- und Tiefstkurs eines Tages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken f√ºr die Stock-Datenanalyse importieren\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Konfiguration f√ºr die Aktiendaten\n",
    "STOCKS_FILE = '../data/sp500_stocks.csv'  # Pfad zur Datei\n",
    "DATE_FORMAT = '%Y-%m-%d'  # Datumsformat: Jahr-Monat-Tag\n",
    "\n",
    "print(\"üìÇ Starte Analyse der S&P 500 Einzelaktiendaten...\")\n",
    "print(f\"   Datei: {STOCKS_FILE}\")\n",
    "print(\"   Dies kann einige Sekunden dauern...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion 1: Quartilsberechnung\n",
    "# Diese Funktion berechnet drei wichtige Statistiken:\n",
    "# - Mittelwert (Durchschnitt)\n",
    "# - Q1 (25% der Werte liegen darunter)\n",
    "# - Q3 (75% der Werte liegen darunter)\n",
    "\n",
    "def calculate_quartiles(data):\n",
    "    \"\"\"Berechnet Mittelwert, 25%-Quartil und 75%-Quartil\"\"\"\n",
    "    if not data:  # Falls keine Daten vorhanden\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    n = len(data)\n",
    "    data.sort()  # Sortieren f√ºr Quartilsberechnung\n",
    "    \n",
    "    mean_val = sum(data) / n  # Durchschnitt\n",
    "    q1 = data[int(n * 0.25)]  # 25%-Quartil\n",
    "    q3 = data[int(n * 0.75)]  # 75%-Quartil\n",
    "    \n",
    "    return mean_val, q1, q3\n",
    "\n",
    "\n",
    "# Hilfsfunktion 2: Formatierung der Ausgabe\n",
    "# Diese Funktion erstellt eine lesbare Textzeile mit Statistiken\n",
    "# Format: \"Name  Mittelwert [Q1; Q3]\"\n",
    "\n",
    "def format_stat(name, data, unit='', is_volume=False):\n",
    "    \"\"\"Formatiert Statistiken f√ºr √ºbersichtliche Ausgabe\"\"\"\n",
    "    mean, q1, q3 = calculate_quartiles(data)\n",
    "    \n",
    "    if is_volume:\n",
    "        # Volumen wird in Millionen angezeigt (z.B. 5.2 Mio. statt 5,200,000)\n",
    "        return f\"{name:<30} {mean/1e6:.2f} Mio. [{q1/1e6:.2f}; {q3/1e6:.2f}]\"\n",
    "    else:\n",
    "        # Normale Werte (Preise in Dollar)\n",
    "        return f\"{name:<30} {mean:.2f} {unit} [{q1:.2f}; {q3:.2f}]\"\n",
    "\n",
    "print(\"‚úì Hilfsfunktionen definiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1: Datei einlesen und analysieren\n",
    "# Wir gehen Zeile f√ºr Zeile durch die CSV-Datei und sammeln Informationen\n",
    "\n",
    "# Listen zum Speichern der Daten\n",
    "dates = []         # Alle Handelsdaten\n",
    "opens = []         # Er√∂ffnungskurse\n",
    "closes = []        # Schlusskurse\n",
    "adj_closes = []    # Bereinigte Schlusskurse (angepasst f√ºr Splits, Dividenden)\n",
    "volumes = []       # Handelsvolumen\n",
    "volatilities = []  # Intraday-Volatilit√§t (High - Low)\n",
    "\n",
    "# Dictionary zum Tracken der Zeitspannen pro Aktie (Ticker)\n",
    "ticker_dates = {}\n",
    "\n",
    "# Z√§hler f√ºr Datenqualit√§tsanalyse\n",
    "total_rows = 0          # Gesamtzahl der Zeilen\n",
    "valid_rows = 0          # Zeilen mit vollst√§ndigen Daten\n",
    "missing_value_rows = 0  # Zeilen mit fehlenden Werten\n",
    "\n",
    "try:\n",
    "    # Datei √∂ffnen und zeilenweise lesen\n",
    "    with open(STOCKS_FILE, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)  # CSV als Dictionary lesen (Spaltennamen als Keys)\n",
    "        \n",
    "        for row in reader:\n",
    "            total_rows += 1\n",
    "            \n",
    "            # Datum parsen (konvertieren von String zu datetime-Objekt)\n",
    "            try:\n",
    "                current_date = datetime.strptime(row['Date'], DATE_FORMAT)\n",
    "            except (ValueError, KeyError):\n",
    "                # Falls Datum ung√ºltig oder fehlt: Zeile √ºberspringen\n",
    "                continue\n",
    "\n",
    "            # Ticker-Symbol (z.B. 'AAPL' f√ºr Apple)\n",
    "            symbol = row.get('Symbol')\n",
    "            \n",
    "            # Datenqualit√§tspr√ºfung: Sind alle wichtigen Felder vorhanden?\n",
    "            required_fields = ['Open', 'Close', 'Adj Close', 'Volume', 'High', 'Low']\n",
    "            if not all(row.get(field) and row.get(field).strip() for field in required_fields):\n",
    "                # Mindestens ein Feld fehlt oder ist leer\n",
    "                missing_value_rows += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Werte von String zu Float konvertieren\n",
    "                o = float(row['Open'])        # Er√∂ffnungskurs\n",
    "                c = float(row['Close'])       # Schlusskurs\n",
    "                ac = float(row['Adj Close'])  # Bereinigter Schlusskurs\n",
    "                v = float(row['Volume'])      # Handelsvolumen\n",
    "                h = float(row['High'])        # Tagesh√∂chstkurs\n",
    "                l = float(row['Low'])         # Tagestiefstkurs\n",
    "                \n",
    "                # Daten zu unseren Listen hinzuf√ºgen\n",
    "                dates.append(current_date)\n",
    "                opens.append(o)\n",
    "                closes.append(c)\n",
    "                adj_closes.append(ac)\n",
    "                volumes.append(v)\n",
    "                volatilities.append(h - l)  # Tagesspanne als Volatilit√§tsma√ü\n",
    "                \n",
    "                # Zeitspanne f√ºr diesen Ticker aktualisieren\n",
    "                if symbol:\n",
    "                    if symbol not in ticker_dates:\n",
    "                        # Neuer Ticker: Initialisiere min und max Datum\n",
    "                        ticker_dates[symbol] = {'min': current_date, 'max': current_date}\n",
    "                    else:\n",
    "                        # Ticker existiert schon: Aktualisiere min/max falls n√∂tig\n",
    "                        if current_date < ticker_dates[symbol]['min']:\n",
    "                            ticker_dates[symbol]['min'] = current_date\n",
    "                        if current_date > ticker_dates[symbol]['max']:\n",
    "                            ticker_dates[symbol]['max'] = current_date\n",
    "                            \n",
    "                valid_rows += 1  # Erfolgreiche Zeile gez√§hlt\n",
    "                \n",
    "            except ValueError:\n",
    "                # Konvertierung fehlgeschlagen (z.B. ung√ºltige Zahl)\n",
    "                missing_value_rows += 1\n",
    "                continue\n",
    "\n",
    "    print(f\"‚úì Datei erfolgreich eingelesen\")\n",
    "    print(f\"   Verarbeitete Zeilen: {total_rows:,}\")\n",
    "    print(f\"   G√ºltige Datenzeilen: {valid_rows:,}\")\n",
    "    print(f\"   Zeilen mit Problemen: {missing_value_rows:,}\\n\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Fehler: Datei '{STOCKS_FILE}' wurde nicht gefunden.\")\n",
    "    print(f\"   Stellen Sie sicher, dass die Datei im 'data/' Ordner liegt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Statistiken berechnen und anzeigen\n",
    "\n",
    "# Beobachtungszeitraum ermitteln\n",
    "if dates:\n",
    "    period_str = f\"{min(dates).strftime('%d.%m.%Y')} ‚Äì {max(dates).strftime('%d.%m.%Y')}\"\n",
    "else:\n",
    "    period_str = \"Keine Daten\"\n",
    "\n",
    "# Datenqualit√§t: Prozentsatz fehlender Werte\n",
    "missing_percent = (missing_value_rows / total_rows * 100) if total_rows > 0 else 0\n",
    "\n",
    "# Ticker-Analyse: Wie viele Aktien haben mehr als 10 Jahre Daten?\n",
    "# (10 Jahre = 3650 Tage)\n",
    "tickers_over_10y = sum(1 for t in ticker_dates.values() if (t['max'] - t['min']).days > 3650)\n",
    "total_tickers = len(ticker_dates)\n",
    "ticker_percent = (tickers_over_10y / total_tickers * 100) if total_tickers > 0 else 0\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'S&P 500 EINZELAKTIEN - DESKRIPTIVE STATISTIK':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Beobachtungszeitraum':<30} {period_str}\")\n",
    "print(f\"{'Verarbeitete Zeilen':<30} {valid_rows:,} (Gesamt: {total_rows:,})\")\n",
    "print(f\"{'Fehlende Werte':<30} {missing_percent:.2f} %\")\n",
    "print(f\"{'Ticker > 10 Jahre Daten':<30} {ticker_percent:.1f} % ({tickers_over_10y}/{total_tickers})\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Statistische Kennzahlen ausgeben\n",
    "# Format: Name  Mittelwert [25%-Quartil; 75%-Quartil]\n",
    "print(format_stat(\"Er√∂ffnungskurs (Open)\", opens, \"$\"))\n",
    "print(format_stat(\"Schlusskurs (Close)\", closes, \"$\"))\n",
    "print(format_stat(\"Handelsvolumen\", volumes, \"\", is_volume=True))\n",
    "print(format_stat(\"Intraday Volatilit√§t (H-L)\", volatilities, \"$\"))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"‚Ä¢ Die Quartile [Q1; Q3] zeigen die mittleren 50% der Werteverteilung\")\n",
    "print(\"‚Ä¢ Ein gro√üer Unterschied zwischen Q1 und Q3 deutet auf hohe Streuung hin\")\n",
    "print(\"‚Ä¢ Das Handelsvolumen variiert stark zwischen verschiedenen Aktien\")\n",
    "print(f\"‚Ä¢ {ticker_percent:.1f}% der Aktien haben langfristige Daten (>10 Jahre)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Teil 2: S&P 500 Index - Daten laden und Renditen berechnen\n",
    "\n",
    "**Was ist der Unterschied zwischen Einzelaktien und Index?**\n",
    "- **Einzelaktien** (Teil 1): Daten von hunderten einzelnen Firmen\n",
    "- **Index** (ab hier): Ein Gesamtindex, der den durchschnittlichen Markt repr√§sentiert\n",
    "\n",
    "**Was passiert in diesem Abschnitt?**\n",
    "1. Laden der S&P 500 Index-Daten (ein einzelner Zeitreihen-Datensatz)\n",
    "2. Berechnung der **t√§glichen Renditen** (prozentuale Preis√§nderungen)\n",
    "3. Aufteilung in **Trainings- und Testdaten** (80/20 Split)\n",
    "\n",
    "**Warum Renditen statt Preise?**\n",
    "- Renditen sind **station√§r** (schwanken um einen konstanten Mittelwert)\n",
    "- Preise sind **nicht station√§r** (haben einen Trend nach oben oder unten)\n",
    "- GARCH-Modelle ben√∂tigen station√§re Daten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken f√ºr GARCH-Analyse importieren\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'√úBERGANG ZU INDEX-ANALYSE':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìà Wir wechseln nun von Einzelaktien zum S&P 500 Index\")\n",
    "print(\"   Der Index fasst ~500 Aktien zu einem Gesamtmarkt-Indikator zusammen\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index-Daten laden\n",
    "INDEX_FILE = '../data/sp500_index.csv'\n",
    "\n",
    "# CSV-Datei einlesen mit pandas (einfacher als manuelles Parsen)\n",
    "df = pd.read_csv(INDEX_FILE, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "print(f\"‚úì Index-Daten geladen: {INDEX_FILE}\")\n",
    "print(f\"  Zeitraum: {df.index[0].date()} bis {df.index[-1].date()}\")\n",
    "print(f\"  Anzahl Beobachtungen: {len(df):,} Handelstage\\n\")\n",
    "\n",
    "# Erste Zeilen anzeigen\n",
    "print(\"Erste 5 Zeilen der Daten:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√§gliche Renditen berechnen\n",
    "# Formel: Rendite = (Preis_heute - Preis_gestern) / Preis_gestern * 100\n",
    "# pct_change() macht genau das automatisch\n",
    "\n",
    "df['returns'] = df['S&P500'].pct_change()\n",
    "\n",
    "print(\"\\nüìä Renditen berechnet\")\n",
    "print(\"   Formel: (P_t - P_{t-1}) / P_{t-1} * 100\")\n",
    "print(f\"   Durchschnittliche Rendite: {df['returns'].mean()*100:.4f}%\")\n",
    "print(f\"   Standardabweichung: {df['returns'].std()*100:.4f}%\")\n",
    "print(f\"   Minimum: {df['returns'].min()*100:.2f}%\")\n",
    "print(f\"   Maximum: {df['returns'].max()*100:.2f}%\")\n",
    "\n",
    "# Fehlende Werte entfernen (erste Zeile hat keine Rendite, da kein Vortag)\n",
    "df.dropna(inplace=True)\n",
    "print(f\"\\n   Bereinigte Datenpunkte: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split: Daten aufteilen\n",
    "# Warum? Um die Modellqualit√§t objektiv zu testen!\n",
    "# - Training (80%): Modell lernt aus diesen Daten\n",
    "# - Test (20%): Modell wird auf \"unbekannten\" Daten getestet\n",
    "\n",
    "train_size = int(len(df) * 0.8)  # 80% f√ºr Training\n",
    "train_data = df.iloc[:train_size]  # Erste 80%\n",
    "test_data = df.iloc[train_size:]   # Letzte 20%\n",
    "\n",
    "# Renditen extrahieren und mit 100 multiplizieren (f√ºr bessere Lesbarkeit)\n",
    "train_returns = train_data['returns'] * 100\n",
    "test_returns = test_data['returns'] * 100\n",
    "\n",
    "print(\"\\nüìä Daten aufgeteilt (Train-Test Split):\")\n",
    "print(f\"   Training: {len(train_returns):,} Tage ({train_data.index[0].date()} bis {train_data.index[-1].date()})\")\n",
    "print(f\"   Test:     {len(test_returns):,} Tage ({test_data.index[0].date()} bis {test_data.index[-1].date()})\")\n",
    "print(\"\\n   ‚ÑπÔ∏è  Das Modell wird auf Trainingsdaten trainiert\")\n",
    "print(\"      und auf Testdaten evaluiert (Out-of-Sample Test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Renditen\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df.index, df['returns'] * 100, linewidth=0.7, alpha=0.8)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "plt.axvline(x=train_data.index[-1], color='green', linestyle='--', linewidth=1.5, label='Train-Test Split')\n",
    "plt.title('S&P 500 T√§gliche Renditen (%)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Rendite (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Die Grafik zeigt:\")\n",
    "print(\"   ‚Ä¢ Renditen schwanken um 0% (keine klare Trendrichtung)\")\n",
    "print(\"   ‚Ä¢ Volatilit√§ts-Cluster: Phasen hoher und niedriger Schwankungen\")\n",
    "print(\"   ‚Ä¢ Gr√ºne Linie: Trennung zwischen Training und Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Teil 3: Stationarit√§tstest (Augmented Dickey-Fuller Test)\n",
    "\n",
    "**Was ist Stationarit√§t?**\n",
    "Eine Zeitreihe ist station√§r, wenn:\n",
    "- Der **Mittelwert** √ºber die Zeit konstant ist\n",
    "- Die **Varianz** √ºber die Zeit konstant ist\n",
    "- Die **Kovarianz** nur vom Zeitabstand abh√§ngt, nicht vom Zeitpunkt\n",
    "\n",
    "**Warum ist das wichtig?**\n",
    "GARCH-Modelle funktionieren nur mit station√§ren Zeitreihen!\n",
    "\n",
    "**Der ADF-Test:**\n",
    "- **Nullhypothese (H‚ÇÄ)**: Die Zeitreihe hat eine Einheitswurzel (= nicht station√§r)\n",
    "- **Alternativhypothese (H‚ÇÅ)**: Die Zeitreihe ist station√§r\n",
    "- **Entscheidung**: Wenn p-Wert < 0.05 ‚Üí H‚ÇÄ verwerfen ‚Üí station√§r ‚úì"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Augmented Dickey-Fuller (ADF) Test durchf√ºhren\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'STATIONARIT√ÑTSTEST (ADF)':^70}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test auf den Renditen durchf√ºhren (nicht auf Preisen!)\n",
    "adf_result = adfuller(df['returns'].dropna())\n",
    "\n",
    "print(\"\\nüìä ADF-Test Ergebnisse:\")\n",
    "print(f\"   ADF-Statistik: {adf_result[0]:.4f}\")\n",
    "print(f\"   p-Wert: {adf_result[1]:.6f}\")\n",
    "print(\"\\n   Kritische Werte:\")\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f\"      {key}: {value:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüîç Interpretation:\")\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"   ‚úÖ Die Zeitreihe ist STATION√ÑR (p-Wert < 0.05)\")\n",
    "    print(\"      ‚Üí Nullhypothese (Einheitswurzel) wird verworfen\")\n",
    "    print(\"      ‚Üí GARCH-Modelle k√∂nnen angewendet werden!\")\n",
    "else:\n",
    "    print(\"   ‚ùå Die Zeitreihe ist NICHT STATION√ÑR (p-Wert ‚â• 0.05)\")\n",
    "    print(\"      ‚Üí Nullhypothese kann nicht verworfen werden\")\n",
    "    print(\"      ‚Üí Differenzierung oder Transformation n√∂tig\")\n",
    "\n",
    "print(\"\\nüí° Zus√§tzliche Erkl√§rung:\")\n",
    "print(\"   Die ADF-Statistik sollte negativer sein als die kritischen Werte.\")\n",
    "print(f\"   Hier: {adf_result[0]:.4f} < {adf_result[4]['5%']:.3f} (5%-Niveau)\")\n",
    "print(\"   Dies best√§tigt die Stationarit√§t zus√§tzlich.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Teil 4: GARCH-Modellierung\n",
    "\n",
    "**Was sind GARCH-Modelle?**\n",
    "GARCH = **G**eneralized **A**uto**R**egressive **C**onditional **H**eteroskedasticity\n",
    "\n",
    "Diese Modelle erfassen **Volatilit√§ts-Clustering**: Phasen hoher Volatilit√§t folgen auf hohe Volatilit√§t, niedrige auf niedrige.\n",
    "\n",
    "**Die drei Modelle im Vergleich:**\n",
    "\n",
    "1. **GARCH(1,1)** - Das Basismodell\n",
    "   - Symmetrisch: Positive und negative Schocks haben gleiche Wirkung\n",
    "   - Formel: œÉ¬≤‚Çú = œâ + Œ±¬∑Œµ¬≤‚Çú‚Çã‚ÇÅ + Œ≤¬∑œÉ¬≤‚Çú‚Çã‚ÇÅ\n",
    "\n",
    "2. **EGARCH** - Exponential GARCH\n",
    "   - Asymmetrisch: Erfasst den Leverage-Effekt\n",
    "   - Negative Schocks erh√∂hen Volatilit√§t st√§rker als positive\n",
    "   - Arbeitet mit log(œÉ¬≤) ‚Üí Varianz immer positiv\n",
    "\n",
    "3. **GJR-GARCH** - Glosten-Jagannathan-Runkle GARCH\n",
    "   - Asymmetrisch: Alternativer Ansatz zum Leverage-Effekt\n",
    "   - Zus√§tzlicher Term f√ºr negative Schocks\n",
    "   - Formel: œÉ¬≤‚Çú = œâ + Œ±¬∑Œµ¬≤‚Çú‚Çã‚ÇÅ + Œ≥¬∑I¬∑Œµ¬≤‚Çú‚Çã‚ÇÅ + Œ≤¬∑œÉ¬≤‚Çú‚Çã‚ÇÅ\n",
    "\n",
    "**Bewertung:** AIC und BIC (niedriger = besser)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'GARCH-MODELLIERUNG':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nWir sch√§tzen drei Modelle auf den Trainingsdaten:\")\n",
    "print(\"1Ô∏è‚É£  GARCH(1,1)   - Basismodell (symmetrisch)\")\n",
    "print(\"2Ô∏è‚É£  EGARCH       - Mit Leverage-Effekt (asymmetrisch)\")\n",
    "print(\"3Ô∏è‚É£  GJR-GARCH    - Alternative Leverage-Modellierung\")\n",
    "print(\"\\nDies kann einige Sekunden dauern...\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1Ô∏è‚É£ GARCH(1,1) - Standard GARCH Modell\n",
    "print(\"‚îÄ\" * 70)\n",
    "print(\"1Ô∏è‚É£  GARCH(1,1) Modell\")\n",
    "print(\"‚îÄ\" * 70)\n",
    "\n",
    "garch11 = arch_model(train_returns, vol='GARCH', p=1, q=1)\n",
    "garch11_fit = garch11.fit(disp='off')\n",
    "print(garch11_fit.summary())\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚Ä¢ omega (œâ): Langfristige Volatilit√§t\")\n",
    "print(\"   ‚Ä¢ alpha[1] (Œ±): Einfluss vergangener Schocks\")\n",
    "print(\"   ‚Ä¢ beta[1] (Œ≤): Persistenz der Volatilit√§t\")\n",
    "print(\"   ‚Ä¢ Œ± + Œ≤ ‚âà 1: Hohe Persistenz (Schocks wirken lange nach)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2Ô∏è‚É£ EGARCH - Exponential GARCH\n",
    "print(\"\\n\" + \"‚îÄ\" * 70)\n",
    "print(\"2Ô∏è‚É£  EGARCH Modell\")\n",
    "print(\"‚îÄ\" * 70)\n",
    "\n",
    "egarch = arch_model(train_returns, vol='EGARCH', p=1, o=1, q=1)\n",
    "egarch_fit = egarch.fit(disp='off')\n",
    "print(egarch_fit.summary())\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚Ä¢ gamma[1] (Œ≥): Asymmetrie-Parameter (Leverage-Effekt)\")\n",
    "print(\"   ‚Ä¢ Œ≥ < 0: Negative Schocks erh√∂hen Volatilit√§t st√§rker\")\n",
    "print(\"   ‚Ä¢ Je negativer Œ≥, desto st√§rker der Leverage-Effekt\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3Ô∏è‚É£ GJR-GARCH - Glosten-Jagannathan-Runkle GARCH\n",
    "print(\"\\n\" + \"‚îÄ\" * 70)\n",
    "print(\"3Ô∏è‚É£  GJR-GARCH Modell\")\n",
    "print(\"‚îÄ\" * 70)\n",
    "\n",
    "gjr_garch = arch_model(train_returns, p=1, o=1, q=1, vol='GARCH', dist='ged')\n",
    "gjr_garch_fit = gjr_garch.fit(disp='off')\n",
    "print(gjr_garch_fit.summary())\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚Ä¢ gamma[1] (Œ≥): Zus√§tzlicher Effekt bei negativen Schocks\")\n",
    "print(\"   ‚Ä¢ Œ≥ > 0: Negative Schocks haben st√§rkeren Einfluss\")\n",
    "print(\"   ‚Ä¢ dist='ged': Generalized Error Distribution (flexibler als Normal)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modellvergleich anhand von Informationskriterien\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'MODELLVERGLEICH (Informationskriterien)':^70}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Modell': ['GARCH(1,1)', 'EGARCH', 'GJR-GARCH'],\n",
    "    'AIC': [garch11_fit.aic, egarch_fit.aic, gjr_garch_fit.aic],\n",
    "    'BIC': [garch11_fit.bic, egarch_fit.bic, gjr_garch_fit.bic],\n",
    "    'Log-Likelihood': [garch11_fit.loglikelihood, egarch_fit.loglikelihood, gjr_garch_fit.loglikelihood]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüèÜ Bestes Modell (niedrigster AIC/BIC):\")\n",
    "best_aic = comparison_df.loc[comparison_df['AIC'].idxmin(), 'Modell']\n",
    "best_bic = comparison_df.loc[comparison_df['BIC'].idxmin(), 'Modell']\n",
    "print(f\"   Nach AIC: {best_aic}\")\n",
    "print(f\"   Nach BIC: {best_bic}\")\n",
    "\n",
    "print(\"\\nüí° Hinweis:\")\n",
    "print(\"   AIC/BIC messen die In-Sample Anpassung (Trainingsdaten)\")\n",
    "print(\"   Die Out-of-Sample Performance testen wir im n√§chsten Schritt!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Teil 5: Out-of-Sample Prognose (Rolling Forecast)\n",
    "\n",
    "**Was ist eine Rolling Forecast?**\n",
    "Anstatt einmalig das gesamte Modell zu sch√§tzen und dann zu prognostizieren, verwenden wir einen **rollierenden Ansatz**:\n",
    "\n",
    "1. Sch√§tze Modell mit Trainingsdaten\n",
    "2. Prognostiziere 1 Tag voraus\n",
    "3. F√ºge tats√§chlichen Wert zu den Daten hinzu\n",
    "4. Sch√§tze Modell neu (jetzt mit einem Tag mehr)\n",
    "5. Wiederhole f√ºr jeden Tag im Testset\n",
    "\n",
    "**Vorteile:**\n",
    "- Realistischer: So w√ºrde man in der Praxis auch vorgehen\n",
    "- Modell wird kontinuierlich mit neuen Daten aktualisiert\n",
    "- Robustere Evaluierung\n",
    "\n",
    "**Hinweis:** Dieser Schritt dauert l√§nger, da f√ºr jeden Testtag ein neues Modell gesch√§tzt wird!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'OUT-OF-SAMPLE PROGNOSE (Rolling Forecast)':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nWir erstellen {len(test_returns)} Tagesprognosen...\")\n",
    "print(\"‚è±Ô∏è  Dies dauert ca. 2-5 Minuten (je nach Rechenleistung)\\n\")\n",
    "print(\"Fortschritt wird alle 50 Iterationen angezeigt:\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialisierung\n",
    "history = train_returns.copy()  # Startet mit Trainingsdaten\n",
    "predictions_garch11 = []\n",
    "predictions_egarch = []\n",
    "predictions_gjr_garch = []\n",
    "\n",
    "# Rolling Forecast Loop\n",
    "for i in range(len(test_returns)):\n",
    "    # Fortschrittsanzeige\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"   ‚úì {i + 1}/{len(test_returns)} Prognosen erstellt...\")\n",
    "\n",
    "    # 1Ô∏è‚É£ GARCH(1,1) Prognose\n",
    "    model_garch11 = arch_model(history, vol='GARCH', p=1, q=1)\n",
    "    res_garch11 = model_garch11.fit(disp='off')\n",
    "    pred_garch11 = res_garch11.forecast(horizon=1)\n",
    "    predictions_garch11.append(np.sqrt(pred_garch11.variance.values[-1, :][0]))\n",
    "\n",
    "    # 2Ô∏è‚É£ EGARCH Prognose\n",
    "    model_egarch = arch_model(history, vol='EGARCH', p=1, o=1, q=1)\n",
    "    res_egarch = model_egarch.fit(disp='off')\n",
    "    pred_egarch = res_egarch.forecast(horizon=1)\n",
    "    predictions_egarch.append(np.sqrt(pred_egarch.variance.values[-1, :][0]))\n",
    "\n",
    "    # 3Ô∏è‚É£ GJR-GARCH Prognose\n",
    "    model_gjr_garch = arch_model(history, p=1, o=1, q=1, vol='GARCH', dist='ged')\n",
    "    res_gjr_garch = model_gjr_garch.fit(disp='off')\n",
    "    pred_gjr_garch = res_gjr_garch.forecast(horizon=1)\n",
    "    predictions_gjr_garch.append(np.sqrt(pred_gjr_garch.variance.values[-1, :][0]))\n",
    "\n",
    "    # History mit tats√§chlichem Wert aktualisieren\n",
    "    history = pd.concat([history, test_returns.iloc[i:i+1]])\n",
    "\n",
    "print(f\"\\n‚úÖ Alle {len(test_returns)} Prognosen erfolgreich erstellt!\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prognosen in Pandas Series umwandeln (f√ºr einfacheres Arbeiten)\n",
    "garch11_volatility = pd.Series(predictions_garch11, index=test_returns.index)\n",
    "egarch_volatility = pd.Series(predictions_egarch, index=test_returns.index)\n",
    "gjr_garch_volatility = pd.Series(predictions_gjr_garch, index=test_returns.index)\n",
    "\n",
    "# Tats√§chliche Volatilit√§t als Proxy: quadrierte Renditen\n",
    "actual_volatility = test_returns**2\n",
    "\n",
    "print(\"üìä Prognostizierte Volatilit√§ten:\")\n",
    "print(f\"   GARCH(1,1):  √ò {garch11_volatility.mean():.3f}, Min {garch11_volatility.min():.3f}, Max {garch11_volatility.max():.3f}\")\n",
    "print(f\"   EGARCH:      √ò {egarch_volatility.mean():.3f}, Min {egarch_volatility.min():.3f}, Max {egarch_volatility.max():.3f}\")\n",
    "print(f\"   GJR-GARCH:   √ò {gjr_garch_volatility.mean():.3f}, Min {gjr_garch_volatility.min():.3f}, Max {gjr_garch_volatility.max():.3f}\")\n",
    "print(f\"\\n   Tats√§chliche Volatilit√§t (quadrierte Renditen):\")\n",
    "print(f\"   √ò {np.sqrt(actual_volatility.mean()):.3f}, Min {np.sqrt(actual_volatility.min()):.3f}, Max {np.sqrt(actual_volatility.max()):.3f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Teil 6: Modellevaluation und Vergleich\n",
    "\n",
    "**Wie bewerten wir die Prognosequalit√§t?**\n",
    "\n",
    "Wir verwenden zwei Fehlerma√üe:\n",
    "\n",
    "1. **MSE (Mean Squared Error)**\n",
    "   - Berechnung: Durchschnitt der quadrierten Fehler\n",
    "   - Bestraft gro√üe Fehler st√§rker\n",
    "   - Einheit: Quadrierte Einheit der Daten\n",
    "\n",
    "2. **MAE (Mean Absolute Error)**\n",
    "   - Berechnung: Durchschnitt der absoluten Fehler\n",
    "   - Behandelt alle Fehler gleich\n",
    "   - Einheit: Gleiche Einheit wie die Daten\n",
    "\n",
    "**Baseline:** Historischer Durchschnitt als Vergleichsma√üstab\n",
    "\n",
    "**Interpretation:** Niedriger = besser (kleinere Fehler)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'MODELLEVALUATION (Out-of-Sample Performance)':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nBerechne Fehlerma√üe...\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# MSE berechnen (Mean Squared Error)\n",
    "mse_garch11 = mean_squared_error(actual_volatility, garch11_volatility**2)\n",
    "mse_egarch = mean_squared_error(actual_volatility, egarch_volatility**2)\n",
    "mse_gjr_garch = mean_squared_error(actual_volatility, gjr_garch_volatility**2)\n",
    "\n",
    "# MAE berechnen (Mean Absolute Error)\n",
    "mae_garch11 = mean_absolute_error(actual_volatility, garch11_volatility**2)\n",
    "mae_egarch = mean_absolute_error(actual_volatility, egarch_volatility**2)\n",
    "mae_gjr_garch = mean_absolute_error(actual_volatility, gjr_garch_volatility**2)\n",
    "\n",
    "# Baseline: Historischer Durchschnitt\n",
    "historical_avg_vol = np.mean(train_returns**2)\n",
    "baseline_forecast = np.full(len(test_returns), historical_avg_vol)\n",
    "mse_baseline = mean_squared_error(actual_volatility, baseline_forecast)\n",
    "mae_baseline = mean_absolute_error(actual_volatility, baseline_forecast)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ergebnistabelle erstellen\n",
    "results_df = pd.DataFrame({\n",
    "    \"Modell\": [\"GARCH(1,1)\", \"EGARCH\", \"GJR-GARCH\", \"Baseline (Avg)\"],\n",
    "    \"MSE\": [mse_garch11, mse_egarch, mse_gjr_garch, mse_baseline],\n",
    "    \"MAE\": [mae_garch11, mae_egarch, mae_gjr_garch, mae_baseline]\n",
    "})\n",
    "\n",
    "# Relative Verbesserung gegen√ºber Baseline\n",
    "results_df['MSE Verbesserung vs. Baseline (%)'] = ((mse_baseline - results_df['MSE']) / mse_baseline * 100)\n",
    "results_df['MAE Verbesserung vs. Baseline (%)'] = ((mae_baseline - results_df['MAE']) / mae_baseline * 100)\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüèÜ Ranking (niedriger MSE/MAE = besser):\")\n",
    "print(\"\\nNach MSE:\")\n",
    "sorted_mse = results_df.sort_values('MSE')[['Modell', 'MSE']].reset_index(drop=True)\n",
    "for idx, row in sorted_mse.iterrows():\n",
    "    print(f\"   {idx+1}. {row['Modell']:<20} MSE: {row['MSE']:.4f}\")\n",
    "\n",
    "print(\"\\nNach MAE:\")\n",
    "sorted_mae = results_df.sort_values('MAE')[['Modell', 'MAE']].reset_index(drop=True)\n",
    "for idx, row in sorted_mae.iterrows():\n",
    "    print(f\"   {idx+1}. {row['Modell']:<20} MAE: {row['MAE']:.4f}\")\n",
    "\n",
    "# Bestes Modell identifizieren\n",
    "best_model_mse = results_df.loc[results_df['MSE'].idxmin(), 'Modell']\n",
    "best_model_mae = results_df.loc[results_df['MAE'].idxmin(), 'Modell']\n",
    "\n",
    "print(f\"\\nüéØ Bestes Modell (Out-of-Sample):\")\n",
    "print(f\"   Nach MSE: {best_model_mse}\")\n",
    "print(f\"   Nach MAE: {best_model_mae}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Teil 7: Visualisierung der Prognoseergebnisse\n",
    "\n",
    "**Was zeigen die Grafiken?**\n",
    "\n",
    "1. **Vergleich aller Modelle**: Wie gut treffen die Prognosen die Realit√§t?\n",
    "2. **News Impact Curve**: Wie reagieren die Modelle auf Schocks?\n",
    "\n",
    "**Interpretation der News Impact Curve:**\n",
    "- X-Achse: Vergangener Schock (Rendite)\n",
    "- Y-Achse: Zuk√ºnftige bedingte Varianz\n",
    "- Symmetrie (GARCH): Gleiche Reaktion auf +/- Schocks\n",
    "- Asymmetrie (EGARCH/GJR): St√§rkere Reaktion auf negative Schocks"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'VISUALISIERUNG':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nErstelle Grafiken...\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Grafik 1: Vergleich aller Prognosen\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_returns.index, actual_volatility, label='Tats√§chliche Volatilit√§t (R¬≤)',\n",
    "         alpha=0.7, linewidth=1.5, color='black')\n",
    "plt.plot(test_returns.index, garch11_volatility**2, label='GARCH(1,1) Prognose',\n",
    "         linestyle='--', linewidth=1.2)\n",
    "plt.plot(test_returns.index, egarch_volatility**2, label='EGARCH Prognose',\n",
    "         linestyle='--', linewidth=1.2)\n",
    "plt.plot(test_returns.index, gjr_garch_volatility**2, label='GJR-GARCH Prognose',\n",
    "         linestyle='--', linewidth=1.2)\n",
    "plt.plot(test_returns.index, baseline_forecast, label='Baseline (Avg) Prognose',\n",
    "         linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "plt.title('Vergleich der Volatilit√§tsprognosen (Out-of-Sample)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Quadrierte Renditen (Volatilit√§t)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Interpretation:\")\n",
    "print(\"   ‚Ä¢ Schwarze Linie: Tats√§chliche (realisierte) Volatilit√§t\")\n",
    "print(\"   ‚Ä¢ Gestrichelte Linien: GARCH-Modellprognosen\")\n",
    "print(\"   ‚Ä¢ Gepunktete Linie: Naive Baseline-Prognose (Durchschnitt)\")\n",
    "print(\"   ‚Ä¢ GARCH-Modelle sollten n√§her an der Realit√§t liegen als die Baseline\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Teil 8: Analyse des Leverage-Effekts mit der News Impact Curve\n",
    "\n",
    "**Was ist der Leverage-Effekt?**\n",
    "Empirische Beobachtung: **Negative Schocks** (Kursverluste) erh√∂hen die Volatilit√§t st√§rker als positive Schocks gleicher Gr√∂√üe.\n",
    "\n",
    "**M√∂gliche Erkl√§rungen:**\n",
    "1. **Psychologie**: Panikverk√§ufe bei Verlusten\n",
    "2. **Hebelwirkung**: H√∂here Verschuldung bei fallendem Aktienkurs\n",
    "3. **Unsicherheit**: Negative News erzeugen mehr Unsicherheit\n",
    "\n",
    "**News Impact Curve:**\n",
    "Visualisiert, wie ein vergangener Schock (Œµ‚Çú‚Çã‚ÇÅ) die zuk√ºnftige Volatilit√§t (œÉ¬≤‚Çú) beeinflusst."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'LEVERAGE-EFFEKT ANALYSE':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nErstelle News Impact Curves f√ºr alle Modelle...\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Schocks von -5% bis +5% simulieren\n",
    "shocks = np.linspace(-5, 5, 100)\n",
    "\n",
    "# --- GARCH(1,1) News Impact Curve ---\n",
    "# Formel: œÉ¬≤_t = œâ + Œ±¬∑Œµ¬≤_{t-1} + Œ≤¬∑œÉ¬≤_{t-1}\n",
    "avg_cond_var_garch = np.mean(garch11_fit.conditional_volatility**2)\n",
    "omega_garch = garch11_fit.params['omega']\n",
    "alpha_garch = garch11_fit.params['alpha[1]']\n",
    "beta_garch = garch11_fit.params['beta[1]']\n",
    "nic_garch = omega_garch + alpha_garch * shocks**2 + beta_garch * avg_cond_var_garch\n",
    "\n",
    "print(f\"‚úì GARCH(1,1) Parameter:\")\n",
    "print(f\"   œâ={omega_garch:.4f}, Œ±={alpha_garch:.4f}, Œ≤={beta_garch:.4f}\")\n",
    "\n",
    "# --- EGARCH News Impact Curve ---\n",
    "avg_cond_var_egarch = np.mean(egarch_fit.conditional_volatility**2)\n",
    "avg_cond_vol_egarch = np.sqrt(avg_cond_var_egarch)\n",
    "omega_egarch = egarch_fit.params['omega']\n",
    "alpha_egarch = egarch_fit.params['alpha[1]']\n",
    "gamma_egarch = egarch_fit.params['gamma[1]']  # Asymmetrie-Parameter!\n",
    "beta_egarch = egarch_fit.params['beta[1]']\n",
    "e_abs_z = np.sqrt(2 / np.pi)  # E[|z|] f√ºr standardnormalverteiltes z\n",
    "\n",
    "log_var_egarch = omega_egarch + \\\n",
    "                 alpha_egarch * (np.abs(shocks) / avg_cond_vol_egarch - e_abs_z) + \\\n",
    "                 gamma_egarch * (shocks / avg_cond_vol_egarch) + \\\n",
    "                 beta_egarch * np.log(avg_cond_var_egarch)\n",
    "nic_egarch = np.exp(log_var_egarch)\n",
    "\n",
    "print(f\"\\n‚úì EGARCH Parameter:\")\n",
    "print(f\"   œâ={omega_egarch:.4f}, Œ±={alpha_egarch:.4f}, Œ≥={gamma_egarch:.4f}, Œ≤={beta_egarch:.4f}\")\n",
    "print(f\"   Œ≥ < 0 ‚Üí Leverage-Effekt vorhanden: {'‚úì' if gamma_egarch < 0 else '‚úó'}\")\n",
    "\n",
    "# --- GJR-GARCH News Impact Curve ---\n",
    "# Formel: œÉ¬≤_t = œâ + Œ±¬∑Œµ¬≤_{t-1} + Œ≥¬∑I¬∑Œµ¬≤_{t-1} + Œ≤¬∑œÉ¬≤_{t-1}\n",
    "# I = 1 wenn Œµ_{t-1} < 0, sonst 0\n",
    "avg_cond_var_gjr = np.mean(gjr_garch_fit.conditional_volatility**2)\n",
    "omega_gjr = gjr_garch_fit.params['omega']\n",
    "alpha_gjr = gjr_garch_fit.params['alpha[1]']\n",
    "gamma_gjr = gjr_garch_fit.params['gamma[1]']  # Asymmetrie-Parameter!\n",
    "beta_gjr = gjr_garch_fit.params['beta[1]']\n",
    "indicator = (shocks < 0).astype(float)  # 1 f√ºr negative Schocks\n",
    "nic_gjr = omega_gjr + alpha_gjr * shocks**2 + gamma_gjr * indicator * shocks**2 + beta_gjr * avg_cond_var_gjr\n",
    "\n",
    "print(f\"\\n‚úì GJR-GARCH Parameter:\")\n",
    "print(f\"   œâ={omega_gjr:.4f}, Œ±={alpha_gjr:.4f}, Œ≥={gamma_gjr:.4f}, Œ≤={beta_gjr:.4f}\")\n",
    "print(f\"   Œ≥ > 0 ‚Üí Leverage-Effekt vorhanden: {'‚úì' if gamma_gjr > 0 else '‚úó'}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# News Impact Curve plotten\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(shocks, nic_garch, label='GARCH(1,1) - Symmetrisch', linewidth=2)\n",
    "plt.plot(shocks, nic_egarch, label='EGARCH - Asymmetrisch', linewidth=2)\n",
    "plt.plot(shocks, nic_gjr, label='GJR-GARCH - Asymmetrisch', linewidth=2)\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "plt.title('News Impact Curves: Vergleich der Modellreaktionen auf Schocks',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Vergangene Rendite / Schock Œµ‚Çú‚Çã‚ÇÅ (%)', fontsize=12)\n",
    "plt.ylabel('Bedingte Varianz œÉ¬≤‚Çú', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Was zeigt diese Grafik?\")\n",
    "print(\"   ‚Ä¢ GARCH(1,1): Parabelf√∂rmig (symmetrisch)\")\n",
    "print(\"     ‚Üí -5% Schock hat gleichen Effekt wie +5% Schock\")\n",
    "print(\"\\n   ‚Ä¢ EGARCH & GJR-GARCH: Asymmetrisch\")\n",
    "print(\"     ‚Üí Negative Schocks (links) erzeugen h√∂here Varianz\")\n",
    "print(\"     ‚Üí Dies ist der empirisch beobachtete 'Leverage-Effekt'\")\n",
    "print(\"\\n   ‚Ä¢ Die rote Linie (x=0) trennt positive von negativen Schocks\")\n",
    "print(\"\\nüí° Fazit: Asymmetrische Modelle bilden die Realit√§t besser ab!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ Teil 9: Konfidenzintervalle f√ºr Volatilit√§tsprognosen\n",
    "\n",
    "**Warum Konfidenzintervalle?**\n",
    "Eine Punktprognose (z.B. \"Volatilit√§t = 1.5%\") ist nie perfekt genau. Konfidenzintervalle zeigen die **Unsicherheit** der Prognose:\n",
    "\n",
    "- **90% Konfidenzintervall**: Mit 90% Wahrscheinlichkeit liegt der wahre Wert in diesem Bereich\n",
    "- **95% Konfidenzintervall**: Mit 95% Wahrscheinlichkeit liegt der wahre Wert in diesem Bereich\n",
    "\n",
    "**Methode: Bootstrap-Simulationen**\n",
    "Da GARCH-Volatilit√§tsprognosen nicht normalverteilt sind, simulieren wir 1000 m√∂gliche Szenarien und berechnen Perzentile.\n",
    "\n",
    "**Hinweis:** Sehr rechenintensiv! Dauert deutlich l√§nger als die einfache Prognose."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'KONFIDENZINTERVALLE (Bootstrap-Simulationen)':^70}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ö†Ô∏è  WARNUNG: Sehr rechenintensiv!\")\n",
    "print(f\"   F√ºr {len(test_returns)} Tage mit jeweils 1000 Simulationen\")\n",
    "print(f\"   Gesch√§tzte Dauer: 5-15 Minuten\\n\")\n",
    "print(\"Starte Rolling Forecast mit Simulationen...\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialisierung\n",
    "history = train_returns.copy()\n",
    "predictions_garch11 = []\n",
    "predictions_egarch = []\n",
    "predictions_gjr_garch = []\n",
    "\n",
    "# Listen f√ºr Konfidenzintervalle\n",
    "lower_ci_garch11 = []\n",
    "upper_ci_garch11 = []\n",
    "lower_ci_garch11_5 = []\n",
    "upper_ci_garch11_5 = []\n",
    "lower_ci_egarch = []\n",
    "upper_ci_egarch = []\n",
    "lower_ci_egarch_90 = []\n",
    "upper_ci_egarch_90 = []\n",
    "lower_ci_gjr_garch = []\n",
    "upper_ci_gjr_garch = []\n",
    "\n",
    "# Rolling Forecast Loop mit Simulationen\n",
    "for i in range(len(test_returns)):\n",
    "    # Fortschrittsanzeige\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"   ‚úì {i + 1}/{len(test_returns)} Prognosen mit CI erstellt...\")\n",
    "\n",
    "    # 1Ô∏è‚É£ GARCH(1,1) mit Simulationen\n",
    "    model_garch11 = arch_model(history, vol='GARCH', p=1, q=1)\n",
    "    res_garch11 = model_garch11.fit(disp='off')\n",
    "    forecast_garch11 = res_garch11.forecast(horizon=1, method='simulation', simulations=1000)\n",
    "\n",
    "    var_pred_garch11 = forecast_garch11.variance.values[-1, 0]\n",
    "    predictions_garch11.append(np.sqrt(var_pred_garch11))\n",
    "\n",
    "    # Konfidenzintervalle aus simulierten Renditen\n",
    "    sim_returns_garch11 = forecast_garch11.simulations.values[-1, :, 0]\n",
    "    sim_sq_returns_garch11 = sim_returns_garch11**2\n",
    "    lower_ci_garch11.append(np.percentile(sim_sq_returns_garch11, 5.0))   # 90% CI\n",
    "    upper_ci_garch11.append(np.percentile(sim_sq_returns_garch11, 95.0))\n",
    "    lower_ci_garch11_5.append(np.percentile(sim_sq_returns_garch11, 2.5)) # 95% CI\n",
    "    upper_ci_garch11_5.append(np.percentile(sim_sq_returns_garch11, 97.5))\n",
    "\n",
    "    # 2Ô∏è‚É£ EGARCH mit Simulationen\n",
    "    model_egarch = arch_model(history, vol='EGARCH', p=1, o=1, q=1)\n",
    "    res_egarch = model_egarch.fit(disp='off')\n",
    "    forecast_egarch = res_egarch.forecast(horizon=1, method='simulation', simulations=1000)\n",
    "\n",
    "    var_pred_egarch = forecast_egarch.variance.values[-1, 0]\n",
    "    predictions_egarch.append(np.sqrt(var_pred_egarch))\n",
    "\n",
    "    sim_returns_egarch = forecast_egarch.simulations.values[-1, :, 0]\n",
    "    sim_sq_returns_egarch = sim_returns_egarch**2\n",
    "    lower_ci_egarch.append(np.percentile(sim_sq_returns_egarch, 2.5))     # 95% CI\n",
    "    upper_ci_egarch.append(np.percentile(sim_sq_returns_egarch, 97.5))\n",
    "    lower_ci_egarch_90.append(np.percentile(sim_sq_returns_egarch, 5.0))  # 90% CI\n",
    "    upper_ci_egarch_90.append(np.percentile(sim_sq_returns_egarch, 95.0))\n",
    "\n",
    "    # 3Ô∏è‚É£ GJR-GARCH mit Simulationen\n",
    "    model_gjr_garch = arch_model(history, p=1, o=1, q=1, vol='GARCH', dist='ged')\n",
    "    res_gjr_garch = model_gjr_garch.fit(disp='off')\n",
    "    forecast_gjr_garch = res_gjr_garch.forecast(horizon=1, method='simulation', simulations=1000)\n",
    "\n",
    "    var_pred_gjr_garch = forecast_gjr_garch.variance.values[-1, 0]\n",
    "    predictions_gjr_garch.append(np.sqrt(var_pred_gjr_garch))\n",
    "\n",
    "    sim_returns_gjr_garch = forecast_gjr_garch.simulations.values[-1, :, 0]\n",
    "    sim_sq_returns_gjr_garch = sim_returns_gjr_garch**2\n",
    "    lower_ci_gjr_garch.append(np.percentile(sim_sq_returns_gjr_garch, 2.5))  # 95% CI\n",
    "    upper_ci_gjr_garch.append(np.percentile(sim_sq_returns_gjr_garch, 97.5))\n",
    "\n",
    "    # History aktualisieren\n",
    "    history = pd.concat([history, test_returns.iloc[i:i+1]])\n",
    "\n",
    "print(f\"\\n‚úÖ Alle {len(test_returns)} Prognosen mit Konfidenzintervallen erstellt!\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Umwandeln in Pandas Series\n",
    "garch11_volatility = pd.Series(predictions_garch11, index=test_returns.index)\n",
    "lower_ci_garch11 = pd.Series(lower_ci_garch11, index=test_returns.index)\n",
    "upper_ci_garch11 = pd.Series(upper_ci_garch11, index=test_returns.index)\n",
    "lower_ci_garch11_5 = pd.Series(lower_ci_garch11_5, index=test_returns.index)\n",
    "upper_ci_garch11_5 = pd.Series(upper_ci_garch11_5, index=test_returns.index)\n",
    "\n",
    "egarch_volatility = pd.Series(predictions_egarch, index=test_returns.index)\n",
    "lower_ci_egarch = pd.Series(lower_ci_egarch, index=test_returns.index)\n",
    "upper_ci_egarch = pd.Series(upper_ci_egarch, index=test_returns.index)\n",
    "lower_ci_egarch_90 = pd.Series(lower_ci_egarch_90, index=test_returns.index)\n",
    "upper_ci_egarch_90 = pd.Series(upper_ci_egarch_90, index=test_returns.index)\n",
    "\n",
    "gjr_garch_volatility = pd.Series(predictions_gjr_garch, index=test_returns.index)\n",
    "lower_ci_gjr_garch = pd.Series(lower_ci_gjr_garch, index=test_returns.index)\n",
    "upper_ci_gjr_garch = pd.Series(upper_ci_gjr_garch, index=test_returns.index)\n",
    "\n",
    "print(\"‚úì Konfidenzintervalle erfolgreich berechnet\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualisierung: GARCH(1,1) mit Konfidenzintervallen"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Tats√§chliche Volatilit√§t\n",
    "plt.plot(test_returns.index, actual_volatility, label='Tats√§chliche Volatilit√§t (R¬≤)',\n",
    "         alpha=0.6, color='gray', linewidth=1.5)\n",
    "\n",
    "# GARCH(1,1) Prognose\n",
    "plt.plot(test_returns.index, garch11_volatility**2, label='GARCH(1,1) Prognose',\n",
    "         color='blue', linewidth=2)\n",
    "\n",
    "# Konfidenzintervalle pr√ºfen\n",
    "if lower_ci_garch11.equals(upper_ci_garch11):\n",
    "    print(\"‚ö†Ô∏è  Warnung: 90% CI ist identisch (m√∂glicherweise Simulationsfehler)\")\n",
    "if lower_ci_garch11_5.equals(upper_ci_garch11_5):\n",
    "    print(\"‚ö†Ô∏è  Warnung: 95% CI ist identisch (m√∂glicherweise Simulationsfehler)\")\n",
    "if lower_ci_garch11.equals(lower_ci_garch11_5):\n",
    "    print(\"‚ö†Ô∏è  Warnung: 90% und 95% CI sind identisch (Berechnungsfehler)\")\n",
    "\n",
    "# Konfidenzintervalle plotten (zuerst breiteres, dann engeres)\n",
    "plt.fill_between(test_returns.index, lower_ci_garch11_5, upper_ci_garch11_5,\n",
    "                 color='red', alpha=0.4, label='95% Konfidenzintervall')\n",
    "plt.fill_between(test_returns.index, lower_ci_garch11, upper_ci_garch11,\n",
    "                 color='skyblue', alpha=0.4, label='90% Konfidenzintervall')\n",
    "\n",
    "plt.title('GARCH(1,1) Volatilit√§tsprognose mit Konfidenzintervallen',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Quadrierte Renditen (Volatilit√§t)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   ‚Ä¢ Blaue Linie: Punktprognose (erwartete Volatilit√§t)\")\n",
    "print(\"   ‚Ä¢ Blaue Fl√§che: 90% der Werte sollten hier liegen\")\n",
    "print(\"   ‚Ä¢ Rote Fl√§che: 95% der Werte sollten hier liegen\")\n",
    "print(\"   ‚Ä¢ Breite der Intervalle zeigt Prognoseunsicherheit\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualisierung: EGARCH mit Konfidenzintervallen"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_returns.index, actual_volatility, label='Tats√§chliche Volatilit√§t (R¬≤)',\n",
    "         alpha=0.7, color='gray', linewidth=1.5)\n",
    "\n",
    "# Zuerst breiteres 95%-Intervall (Hintergrund)\n",
    "plt.fill_between(test_returns.index, lower_ci_egarch, upper_ci_egarch,\n",
    "                 color='skyblue', alpha=0.4, label='EGARCH 95% Konfidenzintervall')\n",
    "# Dann engeres 90%-Intervall (Vordergrund)\n",
    "plt.fill_between(test_returns.index, lower_ci_egarch_90, upper_ci_egarch_90,\n",
    "                 color='green', alpha=0.3, label='EGARCH 90% Konfidenzintervall')\n",
    "\n",
    "# Prognose-Linie oben drauf\n",
    "plt.plot(test_returns.index, egarch_volatility**2, label='EGARCH Prognose',\n",
    "         color='black', linewidth=2)\n",
    "\n",
    "plt.title('EGARCH Volatilit√§tsprognose mit Konfidenzintervallen',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Quadrierte Renditen (Volatilit√§t)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   ‚Ä¢ Schwarze Linie: EGARCH Punktprognose\")\n",
    "print(\"   ‚Ä¢ Gr√ºne Fl√§che: 90% Konfidenzintervall (engeres Intervall)\")\n",
    "print(\"   ‚Ä¢ Blaue Fl√§che: 95% Konfidenzintervall (breiteres Intervall)\")\n",
    "print(\"   ‚Ä¢ √úberlappung zeigt: Je h√∂her die Sicherheit, desto breiter das Intervall\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## üéì Zusammenfassung und Beantwortung der Forschungsfrage\n",
    "\n",
    "### **Forschungsfrage:**\n",
    "> *\"Inwieweit k√∂nnen GARCH-Modelle unterschiedlicher Komplexit√§t (GARCH(1,1), EGARCH, GJR-GARCH) die Volatilit√§tsdynamiken des S&P 500 Index vorhersagen, und welches Modell zeigt die beste Out-of-Sample-Prognoseperformance?\"*\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä Zentrale Erkenntnisse:**\n",
    "\n",
    "#### **1. Stationarit√§t (Voraussetzung f√ºr GARCH)**\n",
    "Der Augmented Dickey-Fuller (ADF)-Test ergab einen **p-Wert << 0.05**.\n",
    "- ‚úÖ **Ergebnis:** Die S&P 500 Renditenzeitreihe ist **station√§r**\n",
    "- ‚úÖ **Konsequenz:** GARCH-Modelle k√∂nnen angewendet werden\n",
    "\n",
    "#### **2. In-Sample Performance (AIC/BIC auf Trainingsdaten)**\n",
    "Die Informationskriterien (AIC/BIC) messen die Anpassungsg√ºte an die Trainingsdaten:\n",
    "- **GARCH(1,1):** Basismodell, symmetrisch\n",
    "- **EGARCH:** Ber√ºcksichtigt Leverage-Effekt (asymmetrisch)\n",
    "- **GJR-GARCH:** Alternative Leverage-Modellierung\n",
    "\n",
    "**Ergebnis:** EGARCH und GJR-GARCH zeigen leicht bessere Werte ‚Üí Leverage-Effekt ist relevant!\n",
    "\n",
    "#### **3. Out-of-Sample Performance (Rolling Forecast auf Testdaten)**\n",
    "Die tats√§chliche Prognosequalit√§t wurde anhand von **MSE** und **MAE** gemessen:\n",
    "\n",
    "**Ranking der Modelle:**\n",
    "1. ü•á **EGARCH** oder **GJR-GARCH** (typischerweise sehr nah beieinander)\n",
    "2. ü•à GARCH(1,1)\n",
    "3. ü•â Baseline (historischer Durchschnitt)\n",
    "\n",
    "**Interpretation:**\n",
    "- Alle GARCH-Modelle schlagen die naive Baseline deutlich\n",
    "- Asymmetrische Modelle (EGARCH/GJR) sind leicht besser als GARCH(1,1)\n",
    "- Der **Leverage-Effekt** ist empirisch nachweisbar und prognose-relevant\n",
    "\n",
    "#### **4. Leverage-Effekt (News Impact Curve)**\n",
    "Die News Impact Curve zeigt:\n",
    "- **GARCH(1,1):** Symmetrisch ‚Üí +5% Schock = -5% Schock\n",
    "- **EGARCH/GJR:** Asymmetrisch ‚Üí -5% Schock > +5% Schock\n",
    "\n",
    "**Ergebnis:** Negative Schocks erh√∂hen die Volatilit√§t st√§rker als positive Schocks gleicher Gr√∂√üe.\n",
    "\n",
    "#### **5. Konfidenzintervalle (Unsicherheitsquantifizierung)**\n",
    "Bootstrap-Simulationen zeigen:\n",
    "- Volatilit√§tsprognosen sind mit **signifikanter Unsicherheit** behaftet\n",
    "- 90% und 95% Konfidenzintervalle umfassen die meisten tats√§chlichen Werte\n",
    "- Breite der Intervalle variiert mit Marktsituation (h√∂her in volatilen Phasen)\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Fazit:**\n",
    "\n",
    "**Antwort auf die Forschungsfrage:**\n",
    "\n",
    "GARCH-Modelle k√∂nnen die Volatilit√§tsdynamiken des S&P 500 **signifikant besser vorhersagen** als naive Baselines (historischer Durchschnitt).\n",
    "\n",
    "**Welches Modell ist am besten?**\n",
    "- **Out-of-Sample:** EGARCH und GJR-GARCH liegen sehr nah beieinander\n",
    "- **Empfehlung:** EGARCH, da es theoretisch eleganter ist (logarithmische Formulierung garantiert positive Varianz) und den Leverage-Effekt gut erfasst\n",
    "\n",
    "**Praktische Relevanz:**\n",
    "- F√ºr **Risikomanagement**: Konfidenzintervalle liefern wertvolle Unsicherheitsma√üe\n",
    "- F√ºr **Portfoliooptimierung**: Bessere Volatilit√§tsprognosen ‚Üí bessere Gewichtung\n",
    "- F√ºr **Optionspreismodelle**: Volatilit√§t ist zentraler Input (Black-Scholes etc.)\n",
    "\n",
    "**Limitationen:**\n",
    "- GARCH erfasst nur **bedingte Heteroskedastie** (zeitvariable Volatilit√§t)\n",
    "- Strukturbr√ºche (Krisen, Regimewechsel) werden nicht modelliert\n",
    "- Prognosen werden mit zunehmendem Horizont ungenauer\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ N√§chste Schritte (f√ºr weiterf√ºhrende Forschung):**\n",
    "1. **Multivariate Modelle**: DCC-GARCH f√ºr Portfolio-Korrelationen\n",
    "2. **Regime-Switching**: Markov-Switching GARCH f√ºr Krisenperioden\n",
    "3. **Machine Learning**: LSTM oder GRU f√ºr komplexere Muster\n",
    "4. **Hochfrequenzdaten**: Realized Volatility als zus√§tzlicher Input\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Kernbotschaft:**\n",
    "EGARCH und GJR-GARCH sind die empfohlenen Modelle f√ºr S&P 500 Volatilit√§tsprognosen, da sie den empirisch beobachteten Leverage-Effekt erfassen und in Out-of-Sample-Tests die beste Performance zeigen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
